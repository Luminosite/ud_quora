{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the data for pos_vec_train_0_50000\n",
      "data for pos_vec_train_0_50000 read\n"
     ]
    }
   ],
   "source": [
    "import features_mp as ft\n",
    "t_data = ft.gen_n_feature(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "seed = 42\n",
    "rand_number = seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def one_hot(y):\n",
    "    oc = OneHotEncoder(categories='auto')\n",
    "    y_r = y.reshape(-1,1)\n",
    "    oc.fit(y_r)\n",
    "    r = oc.transform(y_r).toarray()\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_log_loss(y_true, y_pred):\n",
    "    y_p = one_hot(y_pred)\n",
    "    return log_loss(y_true, y_p)\n",
    "\n",
    "\n",
    "def time_cnt(f, tag=\"func\"):\n",
    "    print(\"function '%s' starts\" % tag)\n",
    "    t_start = time()\n",
    "    ret = f()\n",
    "    t_end = time()\n",
    "    t_used = t_end - t_start\n",
    "    print(\"function '%s' use: %f s\" % (tag, t_used))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def prepare_train_set(data, features):\n",
    "    t_data = data[data!=np.inf].dropna()\n",
    "    feature_data = t_data.drop_duplicates(features, keep='last')\n",
    "    input_data = feature_data[features]\n",
    "    input_data = input_data.astype(np.float64)\n",
    "    result = feature_data[['is_duplicate']]\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, result, test_size = 0.2, random_state = 0,\n",
    "                                                        stratify = result)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def dist_features_for(data, q1, q2, tag=\"tfidf\"):\n",
    "    q1 = np.nan_to_num(q1)\n",
    "    q2 = np.nan_to_num(q2)\n",
    "\n",
    "    def add_dist_for(func):\n",
    "        col_name = '{d}_distance_{t}'.format(d=func.__name__, t=tag)\n",
    "        data[col_name] = [func(x, y)  for (x, y) in zip(q1, q2)]\n",
    "        return col_name\n",
    "\n",
    "    feats = []\n",
    "    feats .append( add_dist_for(cosine))\n",
    "    feats .append( add_dist_for(cityblock))\n",
    "    feats .append( add_dist_for(jaccard))\n",
    "    feats .append( add_dist_for(canberra))\n",
    "    feats .append( add_dist_for(euclidean))\n",
    "    feats .append( add_dist_for(minkowski))\n",
    "    feats .append( add_dist_for(braycurtis))\n",
    "\n",
    "    data['skew_q1vec_{t}'.format(t=tag)] = [skew(x) for x in q1]\n",
    "    feats .append( 'skew_q1vec_{t}'.format(t=tag))\n",
    "    data['skew_q2vec_{t}'.format(t=tag)] = [skew(x) for x in q2]\n",
    "    feats .append( 'skew_q2vec_{t}'.format(t=tag))\n",
    "    data['kur_q1vec_{t}'.format(t=tag)] = [kurtosis(x) for x in q1]\n",
    "    feats .append( 'kur_q1vec_{t}'.format(t=tag))\n",
    "    data['kur_q2vec_{t}'.format(t=tag)] = [kurtosis(x) for x in q2]\n",
    "    feats .append( 'kur_q2vec_{t}'.format(t=tag))\n",
    "\n",
    "    return data, feats\n",
    "\n",
    "\n",
    "def prepare_vec_dist_train_set(data, vec_gen, tag=\"tag\"):\n",
    "    vec = time_cnt(vec_gen, tag=\"vec gen for %s\" % tag)\n",
    "    single_set_size = int(vec.shape[0]/2)\n",
    "    q1 = vec[:single_set_size]\n",
    "    q2 = vec[single_set_size:]\n",
    "    \n",
    "    print(\"dist features for %s starts to gen\" % tag)\n",
    "    dist_features_data, features = dist_features_for(data, q1, q2, tag=tag)\n",
    "    \n",
    "    print(\"train sets for %s starts to gen\" % tag)\n",
    "    X_train, X_test, y_train, y_test = prepare_train_set(dist_features_data, features)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def test_data_performace_with_features(data, features):\n",
    "    X_train, X_test, y_train, y_test = prepare_train_set(data, features)\n",
    "    return test_perform_for(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "def test_perform_for(X_train, X_test, y_train, y_test):\n",
    "    rf_clf = RandomForestClassifier(random_state=rand_number)\n",
    "    gbdt_clf = GradientBoostingClassifier(random_state=rand_number)\n",
    "    lr_clf = LogisticRegression(random_state=rand_number)\n",
    "    sgd_clf = SGDClassifier(random_state=rand_number)\n",
    "    xgb_clf = XGBClassifier(random_state=rand_number)\n",
    "    lgb_clf = LGBMClassifier(random_state=rand_number)\n",
    "#     models=[(rf_clf, \"RandomForest\"), (gbdt_clf, \"GBDT\"), (lr_clf, \"LogsitcRegression\"), (sgd_clf, \"SGD\"), \n",
    "#             (xgb_clf, \"XGBoost\")] , (lgb_clf, \"lightGBM\")]\n",
    "#     models=[(xgb_clf, \"XGBoost\"), (lgb_clf, \"lightGBM\")]\n",
    "    models=[(lgb_clf, \"lightGBM\")]\n",
    "\n",
    "    perform = []\n",
    "    for t in models:\n",
    "        model, name = t\n",
    "        t_start = time()\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "        t_end = time()\n",
    "        y_predprob = model.predict_proba(X_train)\n",
    "        print(name, \"training time cost:\", (t_end-t_start))\n",
    "        y_t = model.predict_proba(X_test)\n",
    "        res = [log_loss(y_train, y_predprob), log_loss(y_test, y_t)]\n",
    "        perform.append((name, res))\n",
    "    return perform\n",
    "\n",
    "\n",
    "def tfidf():\n",
    "    ft = ['question1', \"question2\"]\n",
    "    train = t_data.loc[:, ft]\n",
    "    \n",
    "    print('Generate tfidf')\n",
    "    feats= ft\n",
    "    vect_orig = TfidfVectorizer(max_features=None,ngram_range=(1, 1), min_df=3)\n",
    "\n",
    "    corpus = []\n",
    "    for f in feats:\n",
    "        train.loc[:, f] = train.loc[:, f].astype(str)\n",
    "        corpus+=train[f].values.tolist()\n",
    "    vect_orig.fit(corpus)\n",
    "    \n",
    "    train_tfidf = vect_orig.transform(corpus)\n",
    "    return train_tfidf\n",
    "\n",
    "\n",
    "def try_n_for_transfer(transfer, tag=\"svd300\"):\n",
    "    X_train, X_test, y_train, y_test = prepare_vec_dist_train_set(\n",
    "        t_data, lambda: transfer.fit_transform(ti), tag=tag)\n",
    "    performance = test_perform_for(X_train, X_test, y_train, y_test)\n",
    "    return (\"%s performance:\"%tag, performance)\n",
    "    \n",
    "\n",
    "\n",
    "features=[ 'cosine_distance_pca300', 'cityblock_distance_pca300', 'jaccard_distance_pca300',\n",
    "       'canberra_distance_pca300', 'euclidean_distance_pca300', 'minkowski_distance_pca300', 'braycurtis_distance_pca300',\n",
    "       'skew_q1vec_pca300', 'skew_q2vec_pca300', 'kur_q1vec_pca300', 'kur_q2vec_pca300']\n",
    "target_col = \"is_duplicate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate tfidf\n"
     ]
    }
   ],
   "source": [
    "ti = tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'vec gen for TruncatedSVD_50' starts\n",
      "function 'vec gen for TruncatedSVD_50' use: 3.461020 s\n",
      "dist features for TruncatedSVD_50 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for TruncatedSVD_50 starts to gen\n",
      "lightGBM training time cost: 0.6639974117279053\n",
      "function 'vec gen for TruncatedSVD_100' starts\n",
      "function 'vec gen for TruncatedSVD_100' use: 6.968017 s\n",
      "dist features for TruncatedSVD_100 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for TruncatedSVD_100 starts to gen\n",
      "lightGBM training time cost: 0.8199977874755859\n",
      "function 'vec gen for TruncatedSVD_200' starts\n",
      "function 'vec gen for TruncatedSVD_200' use: 15.738045 s\n",
      "dist features for TruncatedSVD_200 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for TruncatedSVD_200 starts to gen\n",
      "lightGBM training time cost: 0.6700310707092285\n",
      "function 'vec gen for TruncatedSVD_300' starts\n",
      "function 'vec gen for TruncatedSVD_300' use: 20.068002 s\n",
      "dist features for TruncatedSVD_300 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for TruncatedSVD_300 starts to gen\n",
      "lightGBM training time cost: 0.6999993324279785\n",
      "function 'vec gen for NMF_10' starts\n",
      "function 'vec gen for NMF_10' use: 4.708004 s\n",
      "dist features for NMF_10 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for NMF_10 starts to gen\n",
      "lightGBM training time cost: 0.6239991188049316\n",
      "function 'vec gen for NMF_20' starts\n",
      "function 'vec gen for NMF_20' use: 11.239999 s\n",
      "dist features for NMF_20 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for NMF_20 starts to gen\n",
      "lightGBM training time cost: 0.6229958534240723\n",
      "function 'vec gen for NMF_30' starts\n",
      "function 'vec gen for NMF_30' use: 32.194003 s\n",
      "dist features for NMF_30 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for NMF_30 starts to gen\n",
      "lightGBM training time cost: 0.6369955539703369\n",
      "function 'vec gen for LatentDirichletAllocation_10' starts\n",
      "function 'vec gen for LatentDirichletAllocation_10' use: 133.117587 s\n",
      "dist features for LatentDirichletAllocation_10 starts to gen\n",
      "train sets for LatentDirichletAllocation_10 starts to gen\n",
      "lightGBM training time cost: 0.581002950668335\n",
      "function 'vec gen for LatentDirichletAllocation_20' starts\n",
      "function 'vec gen for LatentDirichletAllocation_20' use: 131.473919 s\n",
      "dist features for LatentDirichletAllocation_20 starts to gen\n",
      "train sets for LatentDirichletAllocation_20 starts to gen\n",
      "lightGBM training time cost: 0.5700032711029053\n",
      "================= result =================\n",
      "TruncatedSVD_50 performance:\t[('lightGBM', [0.5310311505349541, 0.5665983960828199])]\n",
      "TruncatedSVD_100 performance:\t[('lightGBM', [0.5117958128700166, 0.5571301109523962])]\n",
      "TruncatedSVD_200 performance:\t[('lightGBM', [0.5001674887803823, 0.5495351528648251])]\n",
      "TruncatedSVD_300 performance:\t[('lightGBM', [0.4988523688347255, 0.5380456856277878])]\n",
      "NMF_10 performance:\t[('lightGBM', [0.5539553510150845, 0.5957539627244194])]\n",
      "NMF_20 performance:\t[('lightGBM', [0.5303164085755014, 0.5759068008264504])]\n",
      "NMF_30 performance:\t[('lightGBM', [0.5205100335782367, 0.5631362600287827])]\n",
      "LatentDirichletAllocation_10 performance:\t[('lightGBM', [0.5389515497446643, 0.575780013394681])]\n",
      "LatentDirichletAllocation_20 performance:\t[('lightGBM', [0.5326364405869699, 0.5718166521483227])]\n"
     ]
    }
   ],
   "source": [
    "ms = [\n",
    "      (TruncatedSVD, [50, 100, 200, 300]),\n",
    "      (NMF, [10, 20, 30]),\n",
    "      (LatentDirichletAllocation, [10, 20])\n",
    "     ]\n",
    "reports = []\n",
    "for model_func, n_list in ms:\n",
    "    for n in n_list:\n",
    "        model = model_func(n_components=n)\n",
    "        tag = \"%s_%d\" % (model_func.__name__, n)\n",
    "        reports.append(try_n_for_transfer(model, tag=tag))\n",
    "\n",
    "print(\"================= result =================\")\n",
    "for report in reports:\n",
    "    tags, performances = report\n",
    "    print(\"%s\\t%s\" % (tags, str(performances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'vec gen for nmf100'starts\n",
      "function 'vec gen for nmf100' use: 89.899767 s\n",
      "dist features for nmf100 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/scipy/spatial/distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sets for nmf100 starts to gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest training time cost: 0.5205888748168945\n",
      "[0.6379569677730905, 11.432149677556074]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT training time cost: 1.982990026473999\n",
      "[10.103955327579383, 10.69804031546862]\n",
      "LogsitcRegression training time cost: 0.08344411849975586\n",
      "[11.88595803085617, 11.93105895276114]\n",
      "SGD training time cost: 0.010377168655395508\n",
      "[12.898135566317384, 13.206841242214095]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/kunfu/miniconda2/envs/tf/lib/python3.5/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost training time cost: 1.0862648487091064\n",
      "[10.246515543841523, 10.69804031546862]\n"
     ]
    }
   ],
   "source": [
    "def prepare_vec_dist_train_set(data, vec_gen, tag=\"tag\"):\n",
    "    ms = [\n",
    "          (TruncatedSVD, [50, 100, 200, 300]),\n",
    "          (NMF, [10, 20, 30]),\n",
    "          (LatentDirichletAllocation, [10, 20])\n",
    "         ]\n",
    "    reports = []\n",
    "    for model_func, n_list in ms:\n",
    "        for n in n_list:\n",
    "            model = model_func(n_components=n)\n",
    "            tag = \"%s_%d\" % (model_func.__name__, n)\n",
    "            reports.append(try_n_for_transfer(model, tag=tag))\n",
    "            \n",
    "    vec = time_cnt(vec_gen, tag=\"vec gen for %s\" % tag)\n",
    "    single_set_size = int(vec.shape[0]/2)\n",
    "    q1 = vec[:single_set_size]\n",
    "    q2 = vec[single_set_size:]\n",
    "    \n",
    "    print(\"dist features for %s starts to gen\" % tag)\n",
    "    dist_features_data, features = dist_features_for(data, q1, q2, tag=tag)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_for(data):\n",
    "    ft = ['question1', \"question2\"]\n",
    "    train = data.loc[:, ft]\n",
    "    \n",
    "    feats= ft\n",
    "    vect_orig = TfidfVectorizer(max_features=None,ngram_range=(1, 1), min_df=3)\n",
    "\n",
    "    corpus = []\n",
    "    for f in feats:\n",
    "        train.loc[:, f] = train.loc[:, f].astype(str)\n",
    "        corpus+=train[f].values.tolist()\n",
    "    vect_orig.fit(corpus)\n",
    "    \n",
    "    train_tfidf = vect_orig.transform(corpus)\n",
    "    return train_tfidf\n",
    "\n",
    "\n",
    "\n",
    "def add_vec_features(data, transfer, tag=\"svd300\"):\n",
    "    ti = time_cnt(lambda: tfidf_for(data), tag=\"generate tfidf\")\n",
    "    \n",
    "    vec = time_cnt(lambda: transfer.fit_transform(ti), tag=\"transfer tfidf matrix\")\n",
    "    single_set_size = int(vec.shape[0]/2)\n",
    "    q1 = vec[:single_set_size]\n",
    "    q2 = vec[single_set_size:]\n",
    "    width = int(vec.shape[1])\n",
    "    shortTag = tag.replace(' ', '')\n",
    "    cols = [[\"q{i}_{s}_{sub_num}\".format(i=i, s=shortTag, sub_num=x) for x in range(width)] for i in range(1,3)]\n",
    "    print(cols)\n",
    "    dq1 = pd.DataFrame(q1, columns=cols[0])\n",
    "    dq2 = pd.DataFrame(q2, columns=cols[1])\n",
    "    return pd.concat([data, dq1, dq2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'generate tfidf'starts\n",
      "function 'generate tfidf' use: 1.600985 s\n",
      "function 'transfer tfidf matrix'starts\n",
      "function 'transfer tfidf matrix' use: 12.259087 s\n",
      "[['q1_nmf20_0', 'q1_nmf20_1', 'q1_nmf20_2', 'q1_nmf20_3', 'q1_nmf20_4', 'q1_nmf20_5', 'q1_nmf20_6', 'q1_nmf20_7', 'q1_nmf20_8', 'q1_nmf20_9', 'q1_nmf20_10', 'q1_nmf20_11', 'q1_nmf20_12', 'q1_nmf20_13', 'q1_nmf20_14', 'q1_nmf20_15', 'q1_nmf20_16', 'q1_nmf20_17', 'q1_nmf20_18', 'q1_nmf20_19'], ['q2_nmf20_0', 'q2_nmf20_1', 'q2_nmf20_2', 'q2_nmf20_3', 'q2_nmf20_4', 'q2_nmf20_5', 'q2_nmf20_6', 'q2_nmf20_7', 'q2_nmf20_8', 'q2_nmf20_9', 'q2_nmf20_10', 'q2_nmf20_11', 'q2_nmf20_12', 'q2_nmf20_13', 'q2_nmf20_14', 'q2_nmf20_15', 'q2_nmf20_16', 'q2_nmf20_17', 'q2_nmf20_18', 'q2_nmf20_19']]\n"
     ]
    }
   ],
   "source": [
    "n=20\n",
    "model = NMF(n_components=n, init='random', random_state=seed)\n",
    "\n",
    "data = add_vec_features(t_data, model, tag=\"nmf20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost training time cost: 5.134984493255615\n",
      "lightGBM training time cost: 1.6410515308380127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('XGBoost', [10.065270578321838, 11.047276808112168]),\n",
       " ('lightGBM', [7.274510357774387, 10.327421996744858])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_performace_with_features(data, [\n",
    "    'q1_nmf20_0', 'q1_nmf20_1', 'q1_nmf20_2',\n",
    "       'q1_nmf20_3', 'q1_nmf20_4', 'q1_nmf20_5', 'q1_nmf20_6', 'q1_nmf20_7',\n",
    "       'q1_nmf20_8', 'q1_nmf20_9', 'q1_nmf20_10', 'q1_nmf20_11', 'q1_nmf20_12',\n",
    "       'q1_nmf20_13', 'q1_nmf20_14', 'q1_nmf20_15', 'q1_nmf20_16',\n",
    "       'q1_nmf20_17', 'q1_nmf20_18', 'q1_nmf20_19', 'q2_nmf20_0', 'q2_nmf20_1',\n",
    "       'q2_nmf20_2', 'q2_nmf20_3', 'q2_nmf20_4', 'q2_nmf20_5', 'q2_nmf20_6',\n",
    "       'q2_nmf20_7', 'q2_nmf20_8', 'q2_nmf20_9', 'q2_nmf20_10', 'q2_nmf20_11',\n",
    "       'q2_nmf20_12', 'q2_nmf20_13', 'q2_nmf20_14', 'q2_nmf20_15',\n",
    "       'q2_nmf20_16', 'q2_nmf20_17', 'q2_nmf20_18', 'q2_nmf20_19'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What is the step by step guide to invest in share market in india?',\n",
       "       'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
       "       'How can I increase the speed of my internet connection while using a VPN?',\n",
       "       ...,\n",
       "       \"How can Kaprekar's constant (6174) be proved using MS Excel?\",\n",
       "       'Is Hillary Clinton a dishonest candidate?',\n",
       "       'What is it like to work at a mine in Australia?'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "t_data['question1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in range(1,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"how can kaprekar 's constant ! ( 1=1 ) ( 6174 ) be prove use ms excel ?\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = \"How can Kaprekar's constant !(1=1) (6174) be proved using MS Excel?\"\n",
    "def lemmatize_all(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "        if tag.startswith('NN'):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "' '.join(lemmatize_all(c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
