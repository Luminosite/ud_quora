{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the data for pos_vec_train_0_50000\n",
      "data for pos_vec_train_0_50000 read\n"
     ]
    }
   ],
   "source": [
    "import features_mp as ft\n",
    "t_data = ft.gen_n_feature(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "import sklearn\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from IPython import get_ipython\n",
    "\n",
    "rand_number = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    oc = OneHotEncoder(categories='auto')\n",
    "    y_r = y.reshape(-1,1)\n",
    "    oc.fit(y_r)\n",
    "    r = oc.transform(y_r).toarray()\n",
    "    return r\n",
    "\n",
    "\n",
    "def t_model(gbm):\n",
    "    y_trainp = gbm.predict_proba(X_train)\n",
    "    y_testp  = gbm.predict_proba(X_test)\n",
    "    print(log_loss(y_train, y_trainp), log_loss(y_test, y_testp))\n",
    "\n",
    "\n",
    "def tt_model(gbm):\n",
    "    gbm = gbm.fit(X_train, y_train)\n",
    "    t_model(gbm)\n",
    "\n",
    "out_features = ['question1', 'question2', 'is_duplicate']\n",
    "features = [x for x in t_data.columns if x not in out_features]\n",
    "\n",
    "t_data = t_data[t_data!=np.inf].dropna()\n",
    "feature_data = t_data.drop_duplicates(features, keep='last')\n",
    "input_data = feature_data[features]\n",
    "input_data = input_data.astype(np.float64)\n",
    "result = feature_data[['is_duplicate']]\n",
    "result = result.values.ravel()\n",
    "\n",
    "def time_cnt(f, tag=\"func\"):\n",
    "    t_start = time()\n",
    "    ret = f()\n",
    "    t_end = time()\n",
    "    t_used = t_end - t_start\n",
    "    print(\"function '%s' use: %f s\" % (tag, t_used))\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, result, test_size = 0.2, random_state = 0,\n",
    "                                                    stratify = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_model(model, params):\n",
    "    gs = GridSearchCV(estimator=model, param_grid=params, \n",
    "                  scoring=\"neg_log_loss\", cv=5, n_jobs=6)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(gs.best_params_, gs.best_score_)\n",
    "    t_model(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39808, 212), (9952, 212))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(sklearn.metrics.SCORERS.keys())\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36419910325099414 0.4385182119916422\n"
     ]
    }
   ],
   "source": [
    "# base model\n",
    "model = LGBMClassifier(n_jobs=4, num_thread=4)\n",
    "tt_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  45 out of  45 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 25, 'num_leaves': 50} -0.43267117145260253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params={\n",
    "    'max_depth': [15, 20, 25, 30, 35],\n",
    "    'num_leaves': [20, 35, 50]\n",
    "}\n",
    "\n",
    "gbm = LGBMClassifier(n_jobs=8)\n",
    "\n",
    "gsearch1 = GridSearchCV(gbm, param_grid=params, scoring='neg_log_loss', cv=3, verbose=1, n_jobs=3)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32112639483570904 0.43299972735853454\n"
     ]
    }
   ],
   "source": [
    "best = LGBMClassifier(num_thread=4, n_jobs=4, max_depth=25, num_leaves=50)\n",
    "tt_model(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=3)]: Done  60 out of  60 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_freq': 20, 'learning_rate': 0.1} -0.4338827864115586\n",
      "0.32064693628896285 0.4338503448670902\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'bagging_freq': [20, 35, 50, 70]\n",
    "}\n",
    "gbm = LGBMClassifier(n_jobs=4, max_depth=20, num_leaves=50)\n",
    "\n",
    "gsearch1 = GridSearchCV(gbm, param_grid=params, scoring='neg_log_loss', cv=3, verbose=1, n_jobs=3)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "t_model(gsearch1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   9 out of   9 | elapsed:   24.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_freq': 20} -0.4338827864115586\n",
      "0.32064693628896285 0.4338503448670902\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    'bagging_freq': [20, 19, 18]\n",
    "}\n",
    "gbm = LGBMClassifier(n_jobs=4, max_depth=20, num_leaves=50, learning_rate=0.1)\n",
    "\n",
    "gsearch1 = GridSearchCV(gbm, param_grid=params, scoring='neg_log_loss', cv=3, verbose=1, n_jobs=3)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "t_model(gsearch1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=3)]: Done  75 out of  75 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda_l1': 0, 'lambda_l2': 15} -0.4304202684501281\n",
      "0.3355672871743284 0.4293634599592099\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n",
    "    'lambda_l2': [0, 10, 15, 35, 40],\n",
    "}\n",
    "gbm = LGBMClassifier(n_jobs=8, max_depth=20, num_leaves=50, learning_rate=0.1)\n",
    "\n",
    "gsearch1 = GridSearchCV(gbm, param_grid=params, scoring='neg_log_loss', cv=3, verbose=1, n_jobs=3)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "t_model(gsearch1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda_l1': 0, 'lambda_l2': 15} -0.4304202684501281\n",
      "0.3355672871743284 0.4293634599592099\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    'lambda_l1': [0, 0.03, 0.05, 0.07, 0.09],\n",
    "    'lambda_l2': [10,13,15,18,20],\n",
    "}\n",
    "gbm = LGBMClassifier(n_jobs=8, max_depth=20, num_leaves=50, learning_rate=0.1)\n",
    "\n",
    "gsearch1 = GridSearchCV(gbm, param_grid=params, scoring='neg_log_loss',\n",
    "                        cv=3, verbose=1, n_jobs=4)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "t_model(gsearch1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.6, 'feature_fraction': 0.7} -0.4307992929075793\n",
      "0.33898751509132263 0.4319067963674971\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "#     'cat_smooth': [1, 10, 15, 20, 35]\n",
    "}\n",
    "gbm = LGBMClassifier(n_jobs=4, max_depth=20,num_leaves=50,\n",
    "                     learning_rate=0.1, lambda_l1=0,lambda_l2=15)\n",
    "\n",
    "gsearch1 = GridSearchCV(gbm, param_grid=params,\n",
    "                        scoring='neg_log_loss', cv=3, verbose=1, n_jobs=4)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "t_model(gsearch1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  21 out of  21 | elapsed:   46.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat_smooth': 0.1} -0.4304202684501281\n",
      "0.3355672871743284 0.4293634599592099\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    'cat_smooth': [0.1, 0.3, 0.7, 1, 3, 5, 8]\n",
    "}\n",
    "gbm = LGBMClassifier(n_jobs=3, max_depth=20,num_leaves=50,\n",
    "                     learning_rate=0.1, lambda_l1=0,lambda_l2=15)\n",
    "\n",
    "gsearch1 = GridSearchCV(gbm, param_grid=params, scoring='neg_log_loss',\n",
    "                        cv=3, verbose=1, n_jobs=4)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "t_model(gsearch1.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3355672871743284 0.4293634599592099\n"
     ]
    }
   ],
   "source": [
    "gbm = LGBMClassifier(n_jobs=3, max_depth=20,num_leaves=50,\n",
    "                     learning_rate=0.1, lambda_l1=0,lambda_l2=15)\n",
    "tt_model(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'solver': 'liblinear'} -0.4773198218777107\n",
      "0.4726993214019336 0.4860577805289532\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "param_test_last1 = {\"penalty\": [\"l1\"], \"solver\": ['liblinear']}\n",
    "rl_clf = LogisticRegression()\n",
    "gs = GridSearchCV(estimator=rl_clf, param_grid=param_test_last1,\n",
    "                  scoring='neg_log_loss', cv=5, n_jobs=8)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "print(t_model(gs.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'solver': 'newton-cg'} -0.47885358964912916\n",
      "0.47453683930331875 0.48706166706446935\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "param_test_last2 = {\"penalty\": [\"l2\"], \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag']}\n",
    "rl_clf = LogisticRegression()\n",
    "gs = GridSearchCV(estimator=rl_clf, param_grid=param_test_last2,\n",
    "                  scoring='neg_log_loss', cv=5, n_jobs=8)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "print(t_model(gs.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 15} -0.47418254498830215\n",
      "0.4682089361068081 0.48179063270267747\n"
     ]
    }
   ],
   "source": [
    "rl_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "search_model(rl_clf, {\"C\": [0.01,0.1,1,5,10,15]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 30} -0.47402783285131106\n",
      "0.4679132862017766 0.4810722664705136\n"
     ]
    }
   ],
   "source": [
    "rl_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "search_model(rl_clf, {\"C\": [13, 15, 20, 30, 50]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "search_model(rl_clf, {\"C\": [25 30, 35, 40, 45]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46794262367489087 0.4811051673834153\n"
     ]
    }
   ],
   "source": [
    "rl_clf = LogisticRegression(penalty='l1', solver='liblinear', C=30)\n",
    "tt_model(rl_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.854339969988084 11.92880524169167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=5, penalty='l2',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(n_jobs=5, loss='log',random_state=rand_number)\n",
    "tt_model(sgd_clf)\n",
    "sgd_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'log'} -11.982811848366763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.854339969988084 11.92880524169167\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(n_jobs=5, random_state=rand_number)\n",
    "search_model(sgd_clf, {\"loss\": [\"log\", \"modified_huber\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1'} -11.849631915688299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.337422363458993 10.623319121604244\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(loss=\"log\", n_jobs=3, random_state=rand_number)\n",
    "search_model(sgd_clf, {\"penalty\": ['l2', 'l1', 'elasticnet']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iter': 300, 'tol': 0.0001} -9.930484967268876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.895253021219034 9.134993716067525\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(penalty='l1', loss=\"log\", n_jobs=3,\n",
    "                        random_state=rand_number)\n",
    "search_model(sgd_clf, {\"max_iter\": [50, 100, 200, 300, 400, 500], \n",
    "                       \"tol\": [0.0001, 0.001, 0.01, 0.1, 0.00001]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "S:\\anaconda\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.895253021219034 9.134993716067525\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(penalty='l1', loss=\"log\", max_iter=300, tol=0.0001,\n",
    "                        n_jobs=3, random_state=rand_number)\n",
    "tt_model(sgd_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
